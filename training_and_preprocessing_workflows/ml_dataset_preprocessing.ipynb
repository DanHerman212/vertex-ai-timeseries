{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"setup\"></a>\n",
        "# Environment Setup & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import plot_timeseries as viz\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# set env variable to authenticate to gcp for cloud storage\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/danherman/.jupyter/storage_key.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "02FqvIIMzvG5"
      },
      "outputs": [],
      "source": [
        "# set plot figure size\n",
        "plt.rcParams[\"figure.figsize\"] = (9,6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"loading\"></a>\n",
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "QzGKiHJpqYqM",
        "outputId": "ef87b091-0cd0-4547-afcf-cfed63592048"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File ml_time_series.csv downloaded to ml_dataset.csv.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>arrival_date</th>\n",
              "      <th>duration</th>\n",
              "      <th>mbt</th>\n",
              "      <th>lag_1</th>\n",
              "      <th>lag_2</th>\n",
              "      <th>lag_3</th>\n",
              "      <th>lag_4</th>\n",
              "      <th>lag_5</th>\n",
              "      <th>arrival_idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-04-01 08:54:50</td>\n",
              "      <td>42.17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.25</td>\n",
              "      <td>11.92</td>\n",
              "      <td>14.17</td>\n",
              "      <td>16.00</td>\n",
              "      <td>17.58</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-04-01 09:06:20</td>\n",
              "      <td>34.00</td>\n",
              "      <td>11.50</td>\n",
              "      <td>3.00</td>\n",
              "      <td>4.50</td>\n",
              "      <td>6.50</td>\n",
              "      <td>8.50</td>\n",
              "      <td>10.00</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-04-01 09:20:50</td>\n",
              "      <td>48.40</td>\n",
              "      <td>14.50</td>\n",
              "      <td>9.58</td>\n",
              "      <td>12.67</td>\n",
              "      <td>15.17</td>\n",
              "      <td>17.33</td>\n",
              "      <td>18.83</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-04-01 09:25:53</td>\n",
              "      <td>31.88</td>\n",
              "      <td>5.05</td>\n",
              "      <td>3.00</td>\n",
              "      <td>4.50</td>\n",
              "      <td>6.50</td>\n",
              "      <td>8.50</td>\n",
              "      <td>10.00</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-04-01 09:41:30</td>\n",
              "      <td>35.75</td>\n",
              "      <td>15.62</td>\n",
              "      <td>3.75</td>\n",
              "      <td>5.67</td>\n",
              "      <td>8.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>11.58</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          arrival_date  duration    mbt  lag_1  lag_2  lag_3  lag_4  lag_5  \\\n",
              "0  2021-04-01 08:54:50     42.17    NaN   9.25  11.92  14.17  16.00  17.58   \n",
              "1  2021-04-01 09:06:20     34.00  11.50   3.00   4.50   6.50   8.50  10.00   \n",
              "2  2021-04-01 09:20:50     48.40  14.50   9.58  12.67  15.17  17.33  18.83   \n",
              "3  2021-04-01 09:25:53     31.88   5.05   3.00   4.50   6.50   8.50  10.00   \n",
              "4  2021-04-01 09:41:30     35.75  15.62   3.75   5.67   8.00  10.00  11.58   \n",
              "\n",
              "   arrival_idx  \n",
              "0            1  \n",
              "1            2  \n",
              "2            3  \n",
              "3            4  \n",
              "4            5  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of dataset:(263741, 9)\n"
          ]
        }
      ],
      "source": [
        "# --- Configuration ---\n",
        "bucket_name = 'timeseries-ml-dataset'  #  GCS bucket name\n",
        "source_blob_name = 'ml_time_series.csv' # The name of the file in our GCS bucket\n",
        "destination_file_name = 'ml_dataset.csv' # The name we want for the downloaded file on your local disk/notebook environment\n",
        "\n",
        "# --- Initialize client ---\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# --- Get the bucket and blob (file) ---\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "blob = bucket.blob(source_blob_name)\n",
        "\n",
        "# --- Download the file ---\n",
        "try:\n",
        "    blob.download_to_filename(destination_file_name)\n",
        "    print(f\"File {source_blob_name} downloaded to {destination_file_name}.\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error downloading file: {e}\")\n",
        "\n",
        "# --- we can now load the downloaded file with pandas ---\n",
        "import pandas as pd\n",
        "df = pd.read_csv(destination_file_name)\n",
        "display(df.head())\n",
        "print(f'Shape of dataset:{df.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdHG5n4QslWf"
      },
      "source": [
        "### There are a small number of nulls, we will just remove them as we have a sufficiently dense sampling of arrival times over a 2 year period"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"cleaning\"></a>\n",
        "# Data Cleaning & Outlier Removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVZA_EWyxZaD"
      },
      "source": [
        "### Outlier Analysis & Cleaning\n",
        "\n",
        "We will perform the following steps:\n",
        "1. **Remove Bad Data (Duration)**: Filter out rows with negative `duration` values.\n",
        "2. **Remove Extreme Outliers (Duration)**: Filter out rows where `duration > 55` minutes. A duration > 55 mins implies a massive delay (e.g., 20+ mins in a tunnel), which is considered an error or non-representative event.\n",
        "3. **Remove Extreme Outliers (MBT)**: Filter out rows where `mbt > 35` minutes. These represent ~4% of the data and are considered errors or non-useful extreme events.\n",
        "4. **Analyze Distributions**: Inspect the cleaned data to decide on further transformations (e.g., Log Transform)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF8NO2nhyPTE"
      },
      "outputs": [],
      "source": [
        "# duration\n",
        "# 1. Remove Bad Data: Filter out negative duration instances\n",
        "initial_count = len(df)\n",
        "df = df[df['duration'] >= 0].copy()\n",
        "print(f\"Removed {initial_count - len(df)} rows with negative duration.\")\n",
        "\n",
        "# 2. Remove Extreme Outliers: Filter out duration > 150 minutes\n",
        "# there are 30 instances of duration between 100 - 150 minutes from 2024 - 2025\n",
        "upper_duration_bound = 150\n",
        "initial_count = len(df)\n",
        "df = df[df['duration'] <= upper_duration_bound].copy()\n",
        "print(f\"Removed {initial_count - len(df)} rows where duration > {upper_duration_bound} mins.\")\n",
        "\n",
        "# 3. Analyze distribution of outliers for duration\n",
        "print(\"\\n--- Duration Distribution (Outliers) ---\")\n",
        "# Display quantiles to see the spread, including extremes\n",
        "print(df['duration'].quantile([0.0, 0.001, 0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99, 0.999, 1.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJmer0kRyXjb",
        "outputId": "4d972c71-d170-440c-c044-0ffe9f06bf0b"
      },
      "outputs": [],
      "source": [
        "# 3. Remove Extreme Outliers: Filter out mbt > 35 minutes\n",
        "# User identified that > 35 mins is likely error/garbage data (approx 4% of data)\n",
        "upper_bound = 30\n",
        "initial_count = len(df)\n",
        "df = df[df['mbt'] <= upper_bound].copy()\n",
        "print(f\"Removed {initial_count - len(df)} rows where mbt > {upper_bound} mins.\")\n",
        "\n",
        "# Analyze distribution for mbt and lags AFTER cleaning\n",
        "cols_to_check = ['mbt', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5']\n",
        "\n",
        "print(\"\\n--- MBT & Lags Distribution (After Cleaning) ---\")\n",
        "for col in cols_to_check:\n",
        "    print(f\"\\n{col} Quantiles:\")\n",
        "    print(df[col].quantile([0.0, 0.001, 0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99, 0.999, 1.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mBTgoNq0Yxi"
      },
      "source": [
        "<a id=\"eda\"></a>\n",
        "# Exploratory Data Analysis (EDA)\n",
        "# Visualize Trends in Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "niSo6kk231Qx",
        "outputId": "bab9a4bf-5756-45ba-bf63-ae2fbe913fba"
      },
      "outputs": [],
      "source": [
        "# sort the dates\n",
        "#df = df.sort_values(by='date', ascending=True).reset_index(drop=True)\n",
        "\n",
        "viz.plot_zoomed_slice(df, 'mbt', start_idx=0, end_idx=175)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAUkGExm5rHj"
      },
      "source": [
        "# The Heatmap (The \"Fingerprint\" of Transit)\n",
        "This is the gold standard for transit data. It visualizes the schedule density by Time of Day vs. Day of Week. It will immediately reveal rush hours and weekend schedules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "q2Yn7OqV3-8j",
        "outputId": "d26ec4c2-0c7d-455c-ed9d-802660a5ce57"
      },
      "outputs": [],
      "source": [
        "viz.plot_heatmap(df, 'mbt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "Mxc-A8aE4DdG",
        "outputId": "d19219aa-040f-446f-c30b-0740a8d8ebd4"
      },
      "outputs": [],
      "source": [
        "viz.plot_distribution(df, 'mbt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUanhno8gNpS"
      },
      "source": [
        "<a id=\"nufft\"></a>\n",
        "# Spectral Analysis (NUFFT)\n",
        "### Preprocessing using NUFFT (Non-Uniform Fast Fourier Transform)\n",
        "To better understand the periodic patterns in our transit data, we will apply the Non-Uniform Fast Fourier Transform (NUFFT). This technique is particularly useful for analyzing time series data that may not be uniformly sampled, such as transit arrival times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.signal import lombscargle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from astropy.timeseries import LombScargle\n",
        "\n",
        "# prepare data\n",
        "if 'arrival_date' not in df.columns:\n",
        "    print(\"Error: 'date' column required.\")\n",
        "else:\n",
        "    # convert to days for astropy \n",
        "    t_days = (df['arrival_date'] - df['arrival_date'].min()).dt.total_seconds().values / (24 * 3600)\n",
        "    y_signal = df['mbt'].values\n",
        "\n",
        "    # compute periodogram (autoselects best frequencies)\n",
        "    # autopower automatically determines the frequency grid and uses the fft method\n",
        "    frequency, power = LombScargle(t_days, y_signal).autopower(minimum_frequency=1/365.25, maximum_frequency=24)\n",
        "\n",
        "    # plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # convert frequency (cycles/day) to cycles/year for readability\n",
        "    freq_per_year = frequency * 365.25\n",
        "\n",
        "    plt.plot(freq_per_year, power)\n",
        "    plt.xscale('log')\n",
        "\n",
        "    # mark key periodicities\n",
        "    plt.xticks([1, 52, 365.25], ['1/year', '1/week', '1/day'])\n",
        "\n",
        "    plt.xlabel('Frequency (Cycles Per Year)')\n",
        "    plt.ylabel('Power (Astropy Lomb-Scargle)')\n",
        "    plt.title('Fast Spectral Analysis (Astorpy)')\n",
        "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"features\"></a>\n",
        "# Feature Engineering\n",
        "Our spectral analysis confirmed strong periodicities at 1 day and 1 week, we should explicitly encode these cycles into our dataset.  Specifically, we will convert time features into cyclical features using sine and cosine transformations.<br>\n",
        "<br>\n",
        "Machine learning models (like LSTMs) struggle with raw time inputs (e.g., Hour 0 and Hour 23 look \"far apart\" numerically, but they are actually adjacent). The standard fix is to transform time into Cyclical Features using Sine and Cosine waves.<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert timestamp to seconds to calculate precise angles\n",
        "timestamp_s = df['arrival_date'].map(pd.Timestamp.timestamp)\n",
        "\n",
        "# define periods in seconds\n",
        "day = 24 * 60 * 60\n",
        "week = 7 * day\n",
        "year = (365.2425) * day\n",
        "\n",
        "# 1. encode daily seasonality (captures rush hour vs. evening)\n",
        "df['day_sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
        "df['day_cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
        "\n",
        "# 2. encode weekly seasonality (captures weekday vs.weekend)\n",
        "df['week_sin'] = np.sin(timestamp_s * (2 * np.pi / week))\n",
        "df['week_cos'] = np.cos(timestamp_s * (2 * np.pi / week))\n",
        "\n",
        "# --- NEW: Rolling Regime Features ---\n",
        "# These features explicitly tell the model the \"current speed\" of the system.\n",
        "# This helps the LSTM adjust its expectations for the next step based on recent history\n",
        "# visualize to verify, zoom in on a few days\n",
        "plt.figure(figsize=(14, 4))\n",
        "# plotting first 1000 points (approx 1 week of data)\n",
        "plt.plot(df['day_sin'][:1000], label='Day Sin', alpha=0.7)\n",
        "plt.plot(df['week_sin'][:1000], label='Week Sin', linewidth=3)\n",
        "plt.title(\"Cyclical Time Features (daily vs Weekly)\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Weather Data Integration\n",
        "\n",
        "Weather data is integrated into the ML pipeline to provide additional context for train arrival predictions. By merging weather features (such as temperature, precipitation, snow, visibility and wind speed) with the main dataset, the model can account for external factors that may influence train delays or arrival times. \n",
        "<br><br>\n",
        "This integration is done by the hour, connecting hourly weather data to the closest arrival timestamp.  This procedure ensuring that each train event is enriched with the relevant weather conditions at that time and place. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- WEATHER DATA INTEGRATION (REWRITTEN) ---\n",
        "\n",
        "# 1. Load the weather data\n",
        "weather_df = pd.read_csv('weather_data.csv') \n",
        "\n",
        "# 2. Select only the physics-based features\n",
        "weather_features = ['datetime', 'temp', 'precip', 'snow', 'snowdepth', 'visibility', 'windspeed']\n",
        "weather_df = weather_df[weather_features].copy()\n",
        "\n",
        "# 3. Convert weather timestamp to datetime objects\n",
        "weather_df['datetime'] = pd.to_datetime(weather_df['datetime'])\n",
        "\n",
        "# 4. FIX: Align Date Ranges\n",
        "# Since weather data starts in 2024, we must drop older train data to avoid NaNs.\n",
        "print(f\"Original Train Count: {len(df)}\")\n",
        "df = df[df['arrival_date'] >= '2024-01-01'].copy()\n",
        "print(f\"Filtered Train Count (2024+): {len(df)}\")\n",
        "\n",
        "# 5. Create a \"Join Key\" in your Train Data\n",
        "# Round train arrival time to the nearest hour to match weather data\n",
        "df['weather_join_key'] = df['arrival_date'].dt.round('h')\n",
        "\n",
        "# 6. Merge\n",
        "# Left join ensures we keep all train trips\n",
        "df = pd.merge(df, weather_df, left_on='weather_join_key', right_on='datetime', how='left')\n",
        "\n",
        "# 7. Clean up\n",
        "# Forward fill missing weather data (small gaps)\n",
        "weather_cols = ['temp', 'precip', 'snow', 'snowdepth', 'visibility', 'windspeed']\n",
        "df[weather_cols] = df[weather_cols].fillna(method='ffill')\n",
        "\n",
        "# Drop the helper columns (join key and the duplicate weather timestamp)\n",
        "df.drop(columns=['weather_join_key', 'datetime'], inplace=True)\n",
        "\n",
        "print(\"Merge Complete. New Shape:\", df.shape)\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.iloc[:, 13:19].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rolling Regime Features and Their Purpose\n",
        "\n",
        "Rolling regime features are engineered to capture the recent behavioral patterns of the train system over a moving time window. These features typically include rolling averages, standard deviations, and other statistics calculated over a fixed number of previous time steps (e.g., the last 5 or 10 arrivals). <br><br>\n",
        "The purpose of these regime features is to provide the model with information about short-term trends, volatility, and anomalies in train arrivals, which may not be evident from static features alone. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- REAPPLY ROLLING REGIME FEATURES AFTER WEATHER MERGE ---\n",
        "\n",
        "# These features should be recalculated after merging weather data, to ensure alignment with the final dataset.\n",
        "df['rolling_mean_10'] = df['mbt'].rolling(window=10).mean()\n",
        "df['rolling_std_10'] = df['mbt'].rolling(window=10).std()\n",
        "df['rolling_mean_50'] = df['mbt'].rolling(window=50).mean()\n",
        "# Backfill NaNs generated by the rolling window so we don't lose data\n",
        "df = df.fillna(method='bfill')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ...existing code...\n",
        "# visualize to verify, zoom in on a few days\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Slice the first 1000 points (approx 1 week)\n",
        "slice_idx = 1000\n",
        "x_axis = range(slice_idx)\n",
        "\n",
        "# 1. Plot Raw MBT (The noisy signal)\n",
        "plt.plot(x_axis, df['mbt'][:slice_idx], label='Raw MBT', color='lightgray', alpha=0.5)\n",
        "\n",
        "# 2. Plot Short-Term Trend (Window 10)\n",
        "plt.plot(x_axis, df['rolling_mean_10'][:slice_idx], label='Rolling Mean (10)', color='orange', linewidth=1.5)\n",
        "\n",
        "# 3. Plot Long-Term Trend (Window 50)\n",
        "plt.plot(x_axis, df['rolling_mean_50'][:slice_idx], label='Rolling Mean (50)', color='blue', linewidth=2)\n",
        "\n",
        "plt.title(\"Regime Detection: Raw MBT vs. Rolling Trends\")\n",
        "plt.ylabel(\"Minutes Between Trains\")\n",
        "plt.xlabel(\"Time Steps (Train Arrivals)\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.drop(columns=['lag_1', 'lag_2','lag_3','lag_4','lag_5','arrival_idx'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save dataset to ml_training_workflows\n",
        "\n",
        "df.to_csv('../3_ml_training_workflow_studio/full_clean_ml_datset.csv', header=True, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
